# -*- coding: utf-8 -*-
"""Copy of submission_3_kelas_intermediate_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jrXbR6QXmPNEpaRVIa61P-jTg4Ej2xnE
"""

import os
import random
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
from google.colab import files
from PIL import Image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from collections import defaultdict

"""# Dataset"""

!pip install -q kaggle

files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

"""## Unduh Dataset"""

!kaggle datasets download -d misrakahmed/vegetable-image-dataset

!unzip -q vegetable-image-dataset.zip

path_data = "/content/Vegetable Images/train"

def create_dataset(dir):
    for entri in os.listdir(dir):
        path_entri = os.path.join(dir, entri)
        if os.path.isfile(path_entri):
            os.remove(path_entri)
    print("Dataset Created")

create_dataset(os.path.join(path_data))

data = os.path.join(path_data)
list_data = os.listdir(data)
print(list_data)

"""## Mendapatkan Jumlah Kelas"""

class_size = len(list_data)
print(class_size)

"""## Melihat ukuran dimensi gambar dan Menghitung Jumlah Dataset"""

size_count = defaultdict(int)

total = 0

for x in list_data:
    dir = os.path.join(path_data, x)
    img_files = os.listdir(dir)

    for img_file in img_files:
        img_path = os.path.join(dir, img_file)
        try:
            with Image.open(img_path) as img:
                size_count[img.size] += 1
        except IOError:
            print(f"Tidak dapat membuka {img_file}")

    total += len(img_files)
print("Total :", total)
print("----------------")
print("Ukuran gambar:")
for size, count in size_count.items():
    print(f"- {size} : {count}")

"""## Tampilkan Sample Dataset"""

all_images = []
sizes = defaultdict(list)
sample = 7

for class_name in os.listdir(path_data):
    class_path = os.path.join(path_data, class_name)
    for image_name in os.listdir(class_path):
        image_path = os.path.join(class_path, image_name)
        try:
            with Image.open(image_path) as img:
                size = img.size
                sizes[size].append((image_path, class_name))
        except IOError:
            print(f"Tidak dapat membuka {image_path}")

selected_images = []
for size, images in sizes.items():
    selected_images.append(random.choice(images))
    if len(selected_images) >= sample:
        break

if len(selected_images) < sample:
    print(f"Tidak cukup gambar dengan ukuran unik untuk menampilkan {sample} gambar.")

plt.figure(figsize=(20, 10))
for i, (image_path, class_name) in enumerate(selected_images, 1):
    plt.subplot(4, 5, i)
    img = mpimg.imread(image_path)
    plt.imshow(img)
    plt.title(class_name)
    plt.axis('off')

plt.tight_layout()
plt.show()

"""## Menyiapkan Datagen menggunakan ImageDataGenerator Untuk Dataset"""

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.2
)

validation_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

"""## Split Dataset"""

batch_size = 32
img_height = 224
img_width = 224

train_generator = train_datagen.flow_from_directory(
    data,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical',
    subset='training'
)

validation_generator = validation_datagen.flow_from_directory(
    data,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation'
)

"""# Model

## Buat Model Sequential
"""

model = Sequential([
    Conv2D(64, (3,3), activation='relu', input_shape=(img_width, img_height, 3)),
    MaxPooling2D(2, 2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Dropout(0.5),
    Flatten(),
    Dropout(0.5),
    Dense(512, activation='relu'),
    Dense(class_size, activation='softmax')
])


model.compile(optimizer=tf.optimizers.Adam(),
              loss='categorical_crossentropy',
              metrics = ['accuracy'])

model.summary()

"""## Buat Callback"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.92 and logs.get('val_accuracy')>0.92):
      print("\nAccuracy above 92%, finish training!")
      self.model.stop_training = True


callbacks = [myCallback(), EarlyStopping(monitor='val_accuracy', patience=5, start_from_epoch=20)]

"""## Latih Model"""

history = model.fit(train_generator,
                      epochs = 40,
                      validation_data = validation_generator,
                      verbose = 2,
                      callbacks = callbacks)

"""# Result"""

fig, axs = plt.subplots(1, 2, figsize=(15, 7), gridspec_kw={'width_ratios': [1, 1]})

axs[0].plot(history.history['accuracy'])
axs[0].plot(history.history['val_accuracy'])
axs[0].set_title('Model Accuracy')
axs[0].set_ylabel('Accuracy')
axs[0].set_xlabel('Epoch')
axs[0].legend(['train', 'test'], loc='upper right')

axs[1].plot(history.history['loss'])
axs[1].plot(history.history['val_loss'])
axs[1].set_title('Model Loss')
axs[1].set_ylabel('Loss')
axs[1].set_xlabel('Epoch')
axs[1].legend(['train', 'test'], loc='upper right')

plt.show()

"""# Export Model"""

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with tf.io.gfile.GFile('model.tflite', 'wb') as f:
  f.write(tflite_model)