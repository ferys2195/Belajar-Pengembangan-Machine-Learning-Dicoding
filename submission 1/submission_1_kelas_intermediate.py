# -*- coding: utf-8 -*-
"""Submission 1 - Kelas Intermediate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Op2PQTDCb33yfthAUruioux_oL0iMALH
"""

import pandas as pd
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt

url = 'https://raw.githubusercontent.com/ferys2195/Dataset/main/Emotion_classify_Data.csv'
df = pd.read_csv(url)
len(df)

df.head()

text_column = 'Comment'
label_column = 'Emotion'

"""# Mengatur Dataset"""

df = df[[text_column,label_column]]
df

"""## Exploratory Dataset

Cek Missing Value
"""

df.isna().sum()

"""Cek Duplikasi data"""

print(df.duplicated().sum())

"""## Cleaning Dataset

Ubah nilai pada **label_column** menjadi kolom
"""

rating = pd.get_dummies(df[label_column])
df_baru = pd.concat([df,rating], axis=1)
df_baru = df_baru.drop(columns=label_column)
df_baru

"""Atur nilai text dan label menjadi tipe data numpy array"""

text = df_baru[text_column].values
label = df_baru[df[label_column].unique().tolist()].values

"""Bagi dataset dengan porsi 80% untuk training dan 20% untuk test (80:20)"""

text_latih, text_test, label_latih, label_test = train_test_split(text, label, test_size=0.2)

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

nltk.download('stopwords')
# Define the tokenizer with a regular expression for tokenization
tokenizer = nltk.RegexpTokenizer(r"\w+")
# Get the English stopwords
stop_words = set(stopwords.words('english'))
# Instantiate the Porter Stemmer
stemmer = PorterStemmer()
# Tokenize, remove stopwords, and apply stemming to the training data
text_latih = [
    [stemmer.stem(word.lower()) for word in tokenizer.tokenize(sentence) if word.lower() not in stop_words]
    for sentence in text_latih
]
# Tokenize, remove stopwords, and apply stemming to the test data
text_test = [
    [stemmer.stem(word.lower()) for word in tokenizer.tokenize(sentence) if word.lower() not in stop_words]
    for sentence in text_test
]

"""Tetapkan jumlah kata untuk di tokenisasi"""

word_count = 4000

tokenizer = Tokenizer(num_words=word_count, oov_token='x')
tokenizer.fit_on_texts(text_latih)

sekuens_latih = tokenizer.texts_to_sequences(text_latih)
sekuens_test = tokenizer.texts_to_sequences(text_test)

padded_latih = pad_sequences(sekuens_latih)
padded_test = pad_sequences(sekuens_test)

"""## Membangun Model"""

class_size = len(df[label_column].unique())

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=word_count, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.1),
    tf.keras.layers.Dense(class_size, activation='softmax')
])

model.compile(loss='categorical_crossentropy',optimizer="adam",metrics=['accuracy'])

"""Buat callback untuk untuk memantau pada saat pelatihan model"""

class MonitoringTrainingCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if logs.get('accuracy') is not None and logs.get('val_accuracy') is not None:
            if logs.get('accuracy') >= 0.99 and logs.get('val_accuracy') >= 0.99:
                print("\nAccuracy dan Validation Accuracy mencapai 90%")
                self.model.stop_training = True

es = EarlyStopping(patience = 3, start_from_epoch=15)
callback = [MonitoringTrainingCallback(), es]

"""Latih Model"""

num_epochs = 30
history = model.fit(padded_latih, label_latih, epochs=num_epochs,
                    validation_data=(padded_test, label_test),
                    steps_per_epoch=25,
                    verbose=2,
                    callbacks=callback)

"""## Hasil

### Nilai Akurasi dan Validation Akurasi tertinggi
"""

max_accuracy = max(history.history['accuracy'])
max_val_accuracy = max(history.history['val_accuracy'])
print(f"Max Accuracy : {round(max_accuracy, 4)}")
print(f"Max Validation Accuracy : {round(max_val_accuracy, 4)}")

"""### Grafik Pelatihan Model"""

# Membuat subplot dengan lebar yang berbeda
fig, axs = plt.subplots(1, 2, figsize=(10, 5), gridspec_kw={'width_ratios': [1, 1]})

# Plot Accuracy di subplot pertama
axs[0].plot(history.history['accuracy'])
axs[0].plot(history.history['val_accuracy'])
axs[0].set_title('Model Accuracy')
axs[0].set_ylabel('Accuracy')
axs[0].set_xlabel('Epoch')
axs[0].legend(['train', 'test'], loc='upper right')

# Plot Loss di subplot kedua
axs[1].plot(history.history['loss'])
axs[1].plot(history.history['val_loss'])
axs[1].set_title('Model Loss')
axs[1].set_ylabel('Loss')
axs[1].set_xlabel('Epoch')
axs[1].legend(['train', 'test'], loc='upper right')

plt.show()